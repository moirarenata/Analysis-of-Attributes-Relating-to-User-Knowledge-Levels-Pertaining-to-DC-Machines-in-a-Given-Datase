{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Title: Analysis of Attributes Relating to User Knowledge Levels In a DC Machine Dataset.\n",
    "\n",
    "Introduction:\n",
    "\n",
    "<li> Background Information </li>\n",
    "\n",
    "Knowledge modeling is the process of creating a model of knowledge and/or specifications for a particular process, structure, or entity that is interpretable by a computer. In this case, the dataset models knowledge of students in the field of electrical DC machines that convert direct current electrical energy into mechanical energy through various measures such as study time and study repetition through modeling methods. DC machines are used in applications where a wide range of speeds and precise good regulation is a requirement alongside widespread use of AC machines which use alternating current. <br>\n",
    "\n",
    "<li> Statement of the Question We Wish to Answer </li>\n",
    "Which attributes out of the ones given, best predict user knowledge level (UNS) and how are those attributes correlated amongst themselves?\n",
    "<br><br>\n",
    "\n",
    "<li> Description of the Dataset </li>\n",
    "The User Knowledge Modeling Data Set is found in the UCI Machine Learning Repository and was created by Hamdi Tolga Kahraman, Ilhami Colak, and Seref Sagiroglu in 2009. This data set has 258 rows and contains data of the students' knowledge in the field of electrical DC Machines. It has 6 columns, 5 input attributes (quantitative) and 1 target attribute (qualitative). The authors classified the UNS based on the K-nearest neighbour algorithm, also known as K-NN. Descriptions of attributes are listed in the table below: \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Attribute</th>\n",
    "    <th>Description</th>\n",
    "    <th>Example</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>STG</td>\n",
    "    <td>The degree of study time for goal object materials.</td>\n",
    "    <td>0, 0.18, 0.276</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SCG</td>\n",
    "    <td>The degree of repetition number of user for goal object materials. </td>\n",
    "    <td>0.3, 0.52, 0.259</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>STR</td>\n",
    "    <td>The degree of study time of user for related objects with goal object. </td>\n",
    "    <td>0.12, 0.78, 0.59</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>LPR</td>\n",
    "    <td>The exam performance of user for related objects with goal object.</td>\n",
    "    <td>0.65, 0.81, 0.2</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>PEG</td>\n",
    "    <td>The exam performance of user for goal objects. </td>\n",
    "    <td>0, 0.9, 0.66</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>UNS</td>\n",
    "    <td>The knowledge level of user. </td>\n",
    "    <td>“Very Low”, “Low”, “Middle”, “High”</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.2     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.0.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.0\n",
      "\n",
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘tibble’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘tidyr’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘dplyr’ was built under R version 4.0.2”\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Registered S3 method overwritten by 'GGally':\n",
      "  method from   \n",
      "  +.gg   ggplot2\n",
      "\n",
      "Warning message:\n",
      "“package ‘tidymodels’ was built under R version 4.0.2”\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 0.1.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom    \u001b[39m 0.7.0      \u001b[32m✔\u001b[39m \u001b[34mrecipes  \u001b[39m 0.1.13\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials    \u001b[39m 0.0.9      \u001b[32m✔\u001b[39m \u001b[34mrsample  \u001b[39m 0.0.7 \n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer    \u001b[39m 0.5.4      \u001b[32m✔\u001b[39m \u001b[34mtune     \u001b[39m 0.1.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata\u001b[39m 0.0.2      \u001b[32m✔\u001b[39m \u001b[34mworkflows\u001b[39m 0.2.0 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip  \u001b[39m 0.1.3      \u001b[32m✔\u001b[39m \u001b[34myardstick\u001b[39m 0.0.7 \n",
      "\n",
      "Warning message:\n",
      "“package ‘broom’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘dials’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘infer’ was built under R version 4.0.3”\n",
      "Warning message:\n",
      "“package ‘modeldata’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘parsnip’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘recipes’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘tune’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘workflows’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘yardstick’ was built under R version 4.0.2”\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"ggridges\")\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(readxl)\n",
    "library(GGally)\n",
    "library (tidymodels)\n",
    "library(ggplot2)\n",
    "library(ggridges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Demonstrate that the dataset can be read from the web into R and combine 2 sheets </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New names:\n",
      "* `` -> ...7\n",
      "* `` -> ...8\n",
      "\n",
      "New names:\n",
      "* `` -> ...7\n",
      "* `` -> ...8\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th><th scope=col>...7</th><th scope=col>...8</th><th scope=col>Attribute Information:</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;lgl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>Very Low</td><td>NA</td><td>NA</td><td>STG (The degree of study time for goal object materails),                  </td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.10</td><td>0.24</td><td>0.90</td><td>High    </td><td>NA</td><td>NA</td><td>SCG (The degree of repetition number of user for goal object materails)    </td></tr>\n",
       "\t<tr><td>0.06</td><td>0.06</td><td>0.05</td><td>0.25</td><td>0.33</td><td>Low     </td><td>NA</td><td>NA</td><td>STR (The degree of study time of user for related objects with goal object)</td></tr>\n",
       "\t<tr><td>0.10</td><td>0.10</td><td>0.15</td><td>0.65</td><td>0.30</td><td>Middle  </td><td>NA</td><td>NA</td><td>LPR (The exam performance of user for related objects with goal object)    </td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.08</td><td>0.98</td><td>0.24</td><td>Low     </td><td>NA</td><td>NA</td><td>PEG (The exam performance of user for goal objects)                        </td></tr>\n",
       "\t<tr><td>0.09</td><td>0.15</td><td>0.40</td><td>0.10</td><td>0.66</td><td>Middle  </td><td>NA</td><td>NA</td><td>UNS (The knowledge level of user)                                          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 9\n",
       "\\begin{tabular}{lllllllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS & ...7 & ...8 & Attribute Information:\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct> & <lgl> & <lgl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & Very Low & NA & NA & STG (The degree of study time for goal object materails),                  \\\\\n",
       "\t 0.08 & 0.08 & 0.10 & 0.24 & 0.90 & High     & NA & NA & SCG (The degree of repetition number of user for goal object materails)    \\\\\n",
       "\t 0.06 & 0.06 & 0.05 & 0.25 & 0.33 & Low      & NA & NA & STR (The degree of study time of user for related objects with goal object)\\\\\n",
       "\t 0.10 & 0.10 & 0.15 & 0.65 & 0.30 & Middle   & NA & NA & LPR (The exam performance of user for related objects with goal object)    \\\\\n",
       "\t 0.08 & 0.08 & 0.08 & 0.98 & 0.24 & Low      & NA & NA & PEG (The exam performance of user for goal objects)                        \\\\\n",
       "\t 0.09 & 0.15 & 0.40 & 0.10 & 0.66 & Middle   & NA & NA & UNS (The knowledge level of user)                                          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 9\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;fct&gt; | ...7 &lt;lgl&gt; | ...8 &lt;lgl&gt; | Attribute Information: &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | Very Low | NA | NA | STG (The degree of study time for goal object materails),                   |\n",
       "| 0.08 | 0.08 | 0.10 | 0.24 | 0.90 | High     | NA | NA | SCG (The degree of repetition number of user for goal object materails)     |\n",
       "| 0.06 | 0.06 | 0.05 | 0.25 | 0.33 | Low      | NA | NA | STR (The degree of study time of user for related objects with goal object) |\n",
       "| 0.10 | 0.10 | 0.15 | 0.65 | 0.30 | Middle   | NA | NA | LPR (The exam performance of user for related objects with goal object)     |\n",
       "| 0.08 | 0.08 | 0.08 | 0.98 | 0.24 | Low      | NA | NA | PEG (The exam performance of user for goal objects)                         |\n",
       "| 0.09 | 0.15 | 0.40 | 0.10 | 0.66 | Middle   | NA | NA | UNS (The knowledge level of user)                                           |\n",
       "\n"
      ],
      "text/plain": [
       "  STG  SCG  STR  LPR  PEG  UNS      ...7 ...8\n",
       "1 0.00 0.00 0.00 0.00 0.00 Very Low NA   NA  \n",
       "2 0.08 0.08 0.10 0.24 0.90 High     NA   NA  \n",
       "3 0.06 0.06 0.05 0.25 0.33 Low      NA   NA  \n",
       "4 0.10 0.10 0.15 0.65 0.30 Middle   NA   NA  \n",
       "5 0.08 0.08 0.08 0.98 0.24 Low      NA   NA  \n",
       "6 0.09 0.15 0.40 0.10 0.66 Middle   NA   NA  \n",
       "  Attribute Information:                                                     \n",
       "1 STG (The degree of study time for goal object materails),                  \n",
       "2 SCG (The degree of repetition number of user for goal object materails)    \n",
       "3 STR (The degree of study time of user for related objects with goal object)\n",
       "4 LPR (The exam performance of user for related objects with goal object)    \n",
       "5 PEG (The exam performance of user for goal objects)                        \n",
       "6 UNS (The knowledge level of user)                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#download file here, look for code\n",
    "\n",
    "user_knowledge_data1 <- read_excel(\"data/Data_User_Modeling_Dataset.xls\", sheet = 2)\n",
    "user_knowledge_data2 <- read_excel(\"data/Data_User_Modeling_Dataset.xls\", sheet = 3)\n",
    "user_knowledge_data1$UNS <- as.character(user_knowledge_data1$UNS)\n",
    "user_knowledge_data1$UNS[user_knowledge_data1$UNS == \"very_low\"] <- \"Very Low\"\n",
    "\n",
    "user_knowledge_data <- rbind (user_knowledge_data1,user_knowledge_data2)\n",
    "\n",
    "user_knowledge_data <- user_knowledge_data %>%\n",
    "        mutate(UNS = as_factor(UNS))\n",
    "\n",
    "head(user_knowledge_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Demonstrate the cleaning and wrangling of the data into a tidy format </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>Very Low</td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.10</td><td>0.24</td><td>0.90</td><td>High    </td></tr>\n",
       "\t<tr><td>0.06</td><td>0.06</td><td>0.05</td><td>0.25</td><td>0.33</td><td>Low     </td></tr>\n",
       "\t<tr><td>0.10</td><td>0.10</td><td>0.15</td><td>0.65</td><td>0.30</td><td>Middle  </td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.08</td><td>0.98</td><td>0.24</td><td>Low     </td></tr>\n",
       "\t<tr><td>0.09</td><td>0.15</td><td>0.40</td><td>0.10</td><td>0.66</td><td>Middle  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & Very Low\\\\\n",
       "\t 0.08 & 0.08 & 0.10 & 0.24 & 0.90 & High    \\\\\n",
       "\t 0.06 & 0.06 & 0.05 & 0.25 & 0.33 & Low     \\\\\n",
       "\t 0.10 & 0.10 & 0.15 & 0.65 & 0.30 & Middle  \\\\\n",
       "\t 0.08 & 0.08 & 0.08 & 0.98 & 0.24 & Low     \\\\\n",
       "\t 0.09 & 0.15 & 0.40 & 0.10 & 0.66 & Middle  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | Very Low |\n",
       "| 0.08 | 0.08 | 0.10 | 0.24 | 0.90 | High     |\n",
       "| 0.06 | 0.06 | 0.05 | 0.25 | 0.33 | Low      |\n",
       "| 0.10 | 0.10 | 0.15 | 0.65 | 0.30 | Middle   |\n",
       "| 0.08 | 0.08 | 0.08 | 0.98 | 0.24 | Low      |\n",
       "| 0.09 | 0.15 | 0.40 | 0.10 | 0.66 | Middle   |\n",
       "\n"
      ],
      "text/plain": [
       "  STG  SCG  STR  LPR  PEG  UNS     \n",
       "1 0.00 0.00 0.00 0.00 0.00 Very Low\n",
       "2 0.08 0.08 0.10 0.24 0.90 High    \n",
       "3 0.06 0.06 0.05 0.25 0.33 Low     \n",
       "4 0.10 0.10 0.15 0.65 0.30 Middle  \n",
       "5 0.08 0.08 0.08 0.98 0.24 Low     \n",
       "6 0.09 0.15 0.40 0.10 0.66 Middle  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_knowledge_data_selected <- select(user_knowledge_data, STG:UNS) \n",
    "head(user_knowledge_data_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Randomize the dataset and split it into training and testing data </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval_select_impl(NULL, .vars, expr(c(!!!dots)), include = .include, : object 'user_knowledge_data_randomized' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval_select_impl(NULL, .vars, expr(c(!!!dots)), include = .include, : object 'user_knowledge_data_randomized' not found\nTraceback:\n",
      "1. initial_split(user_knowledge_data_randomized, prop = 0.75, strata = UNS)",
      "2. tidyselect::vars_select(names(data), !!enquo(strata))",
      "3. eval_select_impl(NULL, .vars, expr(c(!!!dots)), include = .include, \n .     exclude = .exclude, strict = .strict, name_spec = unique_name_spec, \n .     uniquely_named = TRUE)"
     ]
    }
   ],
   "source": [
    "set.seed(2000)\n",
    "\n",
    "knowledge_data_split <- initial_split(user_knowledge_data_random, prop = 0.75, strata = UNS )\n",
    "knowledge_data_training <- training(knowledge_data_split)   \n",
    "knowledge_data_testing <- testing(knowledge_data_split)  \n",
    "\n",
    "head(knowledge_data_training)\n",
    "head(knowledge_data_testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 75:25 data split was chosen to ensure that the training set had enough data to inform a robust model, the training set still having enough observations to be of a sufficient size due to the dataset being large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Summarize the training data based on the total count of observations and the average of each attribute </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_knowledge_data_selected_summarize <- user_knowledge_data_selected %>% \n",
    "group_by(UNS) %>%\n",
    "summarize(Count = n(), Average_STG = mean(STG), Average_SCG = mean(SCG), Average_STR = mean(STR), Average_LPR = mean(LPR), Average_PEG = mean(PEG))\n",
    "user_knowledge_data_selected_summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will attempt to find attributes with the least overlap between their distributions for knowledge levels (UNS) as so to act as good predictors in distinguishing between the 4 levels. Summarizing the total observation count gives us an idea of the size of the dataset and the appropriateness of our split ratio while summarizing the means of the values for each attribute per knowledge level allows for quick assement of the biggest differences to identify trends in which attributes have the least overlap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Visualize training data </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 20, repr.plot.height = 15) \n",
    "knowledge_plot_pairs <- user_knowledge_data_randomized %>%\n",
    "                          ggpairs() + \n",
    "                          ggtitle (\"Figure 1: GGPairs Plot For All Attributes\") +\n",
    "                          theme(text = element_text(size = 20))\n",
    "knowledge_plot_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discuss about why we did ggpairs and correlations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Data Analysis </li>\n",
    "\n",
    "Utilizing the box plot medians and distributions, we will attempt to visually identify the attributes that would be useful as predictors due to distinctness in one or more knowledge categories. We will then attempt to refine our selection by looking at the distributions more in depth to identify areas of overlap and exclusivity. Next, we will run K-NN classification models using the selected predictors to evaluate their accuracy in classifying. Finally, we will visualize the correlation between the predictors using a ggpairs plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_knowledge_data_STG <- knowledge_data_training %>%\n",
    "select(STG, UNS)\n",
    "\n",
    "\n",
    "user_knowledge_data_SCG <- knowledge_data_training %>%\n",
    "select(SCG, UNS)\n",
    "\n",
    "\n",
    "user_knowledge_data_STR <- knowledge_data_training %>%\n",
    "select(STR, UNS)\n",
    "\n",
    "\n",
    "user_knowledge_data_LPR <- knowledge_data_training %>%\n",
    "select(LPR, UNS)\n",
    "\n",
    "\n",
    "user_knowledge_data_PEG <- knowledge_data_training %>%\n",
    "select(PEG, UNS)\n",
    "\n",
    "options(repr.plot.height = 10, repr.plot.width = 10)\n",
    "\n",
    "user_knowledge_data_STG$UNS <- factor(user_knowledge_data_STG$UNS, levels = c(\"Very Low\", \"Low\", \"Middle\", \"High\"))\n",
    "STG_box_plot <- ggplot(user_knowledge_data_STG, aes(x = UNS, y = STG)) + \n",
    "                          geom_boxplot() +\n",
    "                          labs(x = \"User Knowledge Score\", y = \"Values\", title= \"Figure 2a: The Degree of Study Time vs User Knowledge Score\") + \n",
    "                          theme(text = element_text(size = 20))\n",
    "STG_box_plot\n",
    "\n",
    "user_knowledge_data_SCG$UNS <- factor(user_knowledge_data_SCG$UNS, levels = c(\"Very Low\", \"Low\", \"Middle\", \"High\"))\n",
    "SCG_box_plot <- ggplot(user_knowledge_data_SCG, aes(x = UNS, y = SCG)) + \n",
    "                          geom_boxplot() +\n",
    "                          labs(x = \"User Knowledge Score\", y = \"Values\", title= \"Figure 2b: SCG Values vs User Knowledge Score\") + \n",
    "                          theme(text = element_text(size = 20))\n",
    "SCG_box_plot\n",
    "\n",
    "user_knowledge_data_STR$UNS <- factor(user_knowledge_data_STR$UNS, levels = c(\"Very Low\", \"Low\", \"Middle\", \"High\"))\n",
    "STR_box_plot <- ggplot(user_knowledge_data_STR, aes(x = UNS, y = STR)) + \n",
    "                          geom_boxplot() +\n",
    "                          labs(x = \"User Knowledge Score\", y = \"Values\", title= \"Figure 2c: STR Values vs User Knowledge Score\") + \n",
    "                          theme(text = element_text(size = 20))\n",
    "STR_box_plot\n",
    "\n",
    "user_knowledge_data_LPR$UNS <- factor(user_knowledge_data_LPR$UNS, levels = c(\"Very Low\", \"Low\", \"Middle\", \"High\"))\n",
    "LPR_box_plot <- ggplot(user_knowledge_data_LPR, aes(x = UNS, y = LPR)) + \n",
    "                          geom_boxplot() +\n",
    "                          labs(x = \"User Knowledge Score\", y = \"Values\", title= \"Figure 2d: LPR Values vs User Knowledge Score\") + \n",
    "                          theme(text = element_text(size = 20))\n",
    "LPR_box_plot\n",
    "\n",
    "user_knowledge_data_PEG$UNS <- factor(user_knowledge_data_PEG$UNS, levels = c(\"Very Low\", \"Low\", \"Middle\", \"High\"))\n",
    "PEG_box_plot <- ggplot(user_knowledge_data_PEG, aes(x = UNS, y = PEG)) + \n",
    "                          geom_boxplot() +\n",
    "                          labs(x = \"User Knowledge Score\", y = \"Values\", title= \"Figure 2e: PEG Values vs User Knowledge Score\") + \n",
    "                          theme(text = element_text(size = 20))\n",
    "PEG_box_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our boxplots, we can see the following trends in each of the predictors:\n",
    "- STG: The medians for all 4 user knowledge categories are very similar, the distributions have significant overlap. This makes it difficult to distinguish between categories and thus would make not be a good predictor in our model. \n",
    "- SCG: Although the medians are very similar, the distributions have more variance and appear to have less overlap. There also seems to be monotonic increase from \"Very Low\" to \"High\" user knowledge scores, thus at this stage we may consider using this predictor in our model.  \n",
    "- STR: Although the medians appear to have some variance, the distributions have significant overlap across all four UNS categories, thus rendering it unsuitable for use in classification.\n",
    "- LPR: The median appears to \"zig-zag\" as it increases across the UNS categories, this is also reflected in the distributions. Thus we are able to distinguish certain categories such as \"Very Low\" to \"Low\" and \"Medium\" to \"High\". Thus rendering this as a good predictor for our model. \n",
    "- PEG: There is obvious and significant differentiation between the four categories, both in terms of the median and overall distribution. This makes for a very good predictor in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check with Cathy if we need box plot or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height = 10, repr.plot.width = 20)\n",
    "\n",
    "ggplot(user_knowledge_data_STG, aes(x = STG, y = UNS, fill = UNS)) + \n",
    "    geom_density_ridges2() +\n",
    "    labs(title = \"Figure 3a: STG vs UNS\", fill = \"User Knowledge Score Categories\") +\n",
    "    theme(text = element_text(size = 20))\n",
    "\n",
    "ggplot(user_knowledge_data_SCG, aes(x = SCG, y = UNS, fill = UNS)) + \n",
    "    geom_density_ridges2() +\n",
    "    labs(title = \"Figure 3b: SCG vs UNS\", fill = \"User Knowledge Score Categories\") +\n",
    "    theme(text = element_text(size = 20))\n",
    "\n",
    "ggplot(user_knowledge_data_STR, aes(x = STR, y = UNS, fill = UNS)) + \n",
    "    geom_density_ridges2() +\n",
    "    labs(title = \"Figure 3c: STR vs UNS\", fill = \"User Knowledge Score Categories\") +\n",
    "    theme(text = element_text(size = 20))\n",
    "\n",
    "ggplot(user_knowledge_data_LPR, aes(x = LPR, y = UNS, fill = UNS)) + \n",
    "    geom_density_ridges2() +\n",
    "    labs(title = \"Figure 3d: LPR vs UNS\", fill = \"User Knowledge Score Categories\") +\n",
    "    theme(text = element_text(size = 20))\n",
    "\n",
    "ggplot(user_knowledge_data_PEG, aes(x = PEG, y = UNS, fill = UNS)) +\n",
    "    geom_density_ridges2() +\n",
    "    labs(title = \"Figure 3e: PEG vs UNS\", fill = \"User Knowledge Score Categories\") +\n",
    "    theme(text = element_text(size = 20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our ridgeplots, we can see the following trends in each of the predictors:\n",
    "- STG: We can see a rough bimodal distribution for all four UNS categories that overlap significantly, thus our initial assessment based on the boxplots of this not being a good predictor is confirmed. \n",
    "- SCG: Contrary to our assessment from the boxplots, we can see in the ggridge that this variable no longer appears to be a good predictor as it very closely resembles a similar distribution as our STG variable, thus we will not include it in our model.  \n",
    "- STR: Confirmming our initial assessment, all four knowledge score distirbutions can be seen to have significant overlap in a unimodal fashion. Thus, we can exclude this predictor completely from our model.\n",
    "- LPR: There appears to be irregularity in the distributions such that we are able to distinguish between certain categories such as \"Very Low\" to \"Low\" and \"Medium\" to \"High\" based on the varying probability densities under certain values. Thus confirming our initial assessment and can include this variable in our predictive model. \n",
    "- PEG: From the ggridge, we can see slight overlap in the distributions; however, there is still significant deliniation between the four categories. This confirming our intial assessment with the boxplots and making for a very good predictor in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we are able to select 2 predictors, PEG and LPR. However, PEG has a significant difference in the distribution for each knowledge category and we would like to confirm that it is not single handedly responsible for any observed classification accuracy. Thus, we will formulate and run 2 classification models: one utilizing only PEG, the other utilizing both PEG and LPR to evaluate the respective contributions of each predictor to the observed accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2000)\n",
    "\n",
    "peg_knowledge_recipe <- recipe(UNS ~ PEG, data = knowledge_data_training)\n",
    "\n",
    "peg_knowledge_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "       set_engine(\"kknn\") %>%\n",
    "       set_mode(\"classification\")\n",
    "\n",
    "peg_knowledge_vfold <- vfold_cv(knowledge_data_training, v = 5, strata = UNS)\n",
    "\n",
    "peg_knowledge_results <- workflow() %>%\n",
    "       add_recipe(peg_knowledge_recipe) %>%\n",
    "       add_model(peg_knowledge_tune) %>%\n",
    "       tune_grid(resamples = peg_knowledge_vfold, grid = 10) %>%\n",
    "       collect_metrics()\n",
    "\n",
    "options(repr.plot.height = 5, repr.plot.width = 10)\n",
    "\n",
    "peg_accuracies <- peg_knowledge_results %>% \n",
    "       filter(.metric == \"accuracy\")\n",
    "\n",
    "peg_accuracies_plot <- ggplot(peg_accuracies, aes(x = neighbors, y = mean)) +\n",
    "       geom_point() +\n",
    "       geom_line() +\n",
    "       labs(x = \"Neighbors\", y = \"Accuracy Estimate\", title = \"Figure 4: Accuracies Plot based on PEG \") +\n",
    "       scale_x_continuous(breaks = seq(0, 14, by = 1)) +  \n",
    "       scale_y_continuous(limits = c(0.4, 1.0)) + \n",
    "       theme(text = element_text(size = 20)) \n",
    "\n",
    "peg_accuracies_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this accuracy plot, only using one variable (PEG), we can see that the optimized k (neighbor) = 3 at roughly 85% accuracy. Although it appears that k=4 gives the high accuracy estimate, we did not choose k=4 to avoid an even split between the neighboring labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try make k values from 1-100 and multiple of 10 for the k values (10, 20, 30, 40, .., 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2000)\n",
    "\n",
    "peg_knowledge_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 3) %>%\n",
    "       set_engine(\"kknn\") %>%\n",
    "       set_mode(\"classification\")\n",
    "\n",
    "peg_knowledge_fit <- workflow() %>%\n",
    "       add_recipe(peg_knowledge_recipe) %>%\n",
    "       add_model(peg_knowledge_spec) %>%\n",
    "       fit(data = knowledge_data_testing)\n",
    "peg_knowledge_fit\n",
    "                                   \n",
    "peg_knowledge_predictions <- predict(peg_knowledge_fit, knowledge_data_testing) %>%\n",
    "       bind_cols(knowledge_data_testing)\n",
    "head(peg_knowledge_predictions)\n",
    "\n",
    "peg_knowledge_metrics <- peg_knowledge_predictions %>%\n",
    "       metrics(truth = UNS, estimate = .pred_class)\n",
    "peg_knowledge_metrics\n",
    "\n",
    "peg_knowledge_conf_mat <- peg_knowledge_predictions %>%\n",
    "       conf_mat(truth = UNS, estimate = .pred_class)\n",
    "peg_knowledge_conf_mat\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From fiting our model with using k (neighbors) = 3 and only using PEG as our predictor, our predictions (classifier) garner a 89.9% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix indicates relatively few mismatches and any that do occur are not extreme, e.g. misclassifications from Middle to Low occur, but not from Middle to Very Low or from Very High to Low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2000)\n",
    "\n",
    "knowledge_recipe <- recipe(UNS ~ PEG + LPR, data = knowledge_data_training)\n",
    "\n",
    "knowledge_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "       set_engine(\"kknn\") %>%\n",
    "       set_mode(\"classification\")\n",
    "\n",
    "knowledge_vfold <- vfold_cv(knowledge_data_training, v = 5, strata = UNS)\n",
    "\n",
    "knowledge_results <- workflow() %>%\n",
    "       add_recipe(knowledge_recipe) %>%\n",
    "       add_model(knowledge_tune) %>%\n",
    "       tune_grid(resamples = knowledge_vfold, grid = 10) %>%\n",
    "       collect_metrics()\n",
    "\n",
    "options(repr.plot.height = 5, repr.plot.width = 10)\n",
    "\n",
    "accuracies <- knowledge_results %>% \n",
    "       filter(.metric == \"accuracy\")\n",
    "\n",
    "accuracies_plot <- ggplot(accuracies, aes(x = neighbors, y = mean)) +\n",
    "       geom_point() +\n",
    "       geom_line() +\n",
    "       labs(x = \"Neighbors\", y = \"Accuracy Estimate\", title = \"Figure 5: Accuracies Plot based on PEG and LPR\") +\n",
    "       scale_x_continuous(breaks = seq(0, 14, by = 1)) +  \n",
    "       scale_y_continuous(limits = c(0.4, 1.0)) + \n",
    "       theme(text = element_text(size = 20)) \n",
    " \n",
    "\n",
    "accuracies_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this accuracy plot, using two predictors (PEG and LPR) we can see that the optimized k (neighbors) = 3 at roughly 97% accuracy. We could pick any k value between 3-6, but we decided to choose an odd number to avoid an even split between the neighboring labels and a smaller number to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try change the k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2000)\n",
    "\n",
    "knowledge_recipe <- recipe(UNS ~ PEG + LPR, data = knowledge_data_training)\n",
    "\n",
    "knowledge_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 3) %>%\n",
    "       set_engine(\"kknn\") %>%\n",
    "       set_mode(\"classification\")\n",
    "\n",
    "knowledge_fit <- workflow() %>%\n",
    "       add_recipe(knowledge_recipe) %>%\n",
    "       add_model(knowledge_spec) %>%\n",
    "       fit(data = knowledge_data_testing)\n",
    "knowledge_fit\n",
    "                                   \n",
    "knowledge_predictions <- predict(knowledge_fit, knowledge_data_testing) %>%\n",
    "       bind_cols(knowledge_data_testing)\n",
    "head(knowledge_predictions)\n",
    "\n",
    "knowledge_metrics <- knowledge_predictions %>%\n",
    "       metrics(truth = UNS, estimate = .pred_class)\n",
    "knowledge_metrics\n",
    "\n",
    "knowledge_conf_mat <- knowledge_predictions %>%\n",
    "       conf_mat(truth = UNS, estimate = .pred_class)\n",
    "knowledge_conf_mat\n",
    "                                   \n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous model, the confusion matrix indicates relatively few mismatches and any that do occur are not extreme, e.g. misclassifications from Middle to Low occur, but not from Middle to Very Low or from Very High to Low or vice versa. The number of misclassifications is also reduced across the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height = 10, repr.plot.width = 10)\n",
    "\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 3) %>%\n",
    "    set_engine(\"kknn\") %>%\n",
    "    set_mode(\"classification\")\n",
    "  \n",
    "knn_fit <- workflow() %>%\n",
    "    add_recipe(knowledge_recipe) %>%\n",
    "    add_model(knn_spec) %>%\n",
    "    fit(data = knowledge_data_testing)\n",
    "\n",
    "peg_grid <- seq(min(knowledge_data_testing$PEG), max(knowledge_data_testing$PEG), length.out = 100)\n",
    "lpr_grid <- seq(min(knowledge_data_testing$LPR), max(knowledge_data_testing$LPR), length.out = 100)\n",
    "scgrid <- as_tibble(expand.grid(PEG = peg_grid, LPR = lpr_grid))\n",
    "knnPredGrid <- predict(knn_fit, scgrid)\n",
    "prediction_table <- bind_cols(knnPredGrid, scgrid) %>% rename(UNS = .pred_class)\n",
    "\n",
    "knowledge_predictions$UNS <- factor(knowledge_predictions$UNS, levels = c(\"Very Low\", \"Low\", \"Middle\", \"High\"))\n",
    "\n",
    "knowledge_prediction_plot <- ggplot() +\n",
    "    geom_point(data = knowledge_predictions, mapping = aes(x = PEG, y = LPR, color = UNS), alpha= 0.7) +\n",
    "    geom_point(data = prediction_table, mapping = aes(x = PEG, y = LPR, color = UNS), alpha = 0.03, size = 5.) +\n",
    "    labs(color = \"User Knowledge Score\") +\n",
    "    ggtitle(\"Figure 6: PEG and LPR vs User Knowledge Score\") + \n",
    "    theme(text = element_text(size = 20))\n",
    "\n",
    "\n",
    " \n",
    "knowledge_prediction_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot identifies the solution spaces for the classification by utilizing LPR and PEG as predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From fiting our model with k (neighbors) = 3 and using PEG and LPR instead of just PEG as our predictor, our classifier increased from a 85% to a 97% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using only PEG as our predictor, we saw a k value of 5 to be optimal from the accuracy plot and obtained a prediction accuracy of 85%. This agrees with our initial assessment of PEG being an excellent predictor due to the clear deliniation between UNS categories. \n",
    "\n",
    "When adding LPR as a predictor in addition to PEG, we saw many similar k values ranging from 3 to 14, but came to the conclusion that 3 offered the highest accuracy. From this we obtained a prediction accuracy of 97%, an increase of 12% from our original model which only used PEG. Thus, LPR was able to better allow our model to differentiate between categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_plot_pairs <- user_knowledge_data_randomized %>%\n",
    "  select(PEG, LPR) %>%\n",
    "  ggpairs() +\n",
    "  ggtitle (\"Figure 7: GGPairs Between PEG and LPR\") +\n",
    "  theme(text = element_text(size = 20))\n",
    "predictor_plot_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "<li> Summary of results \n",
    "    \n",
    "Results indicated PEG and LPR to be excellent predictors of UNS in a K-NN classification model with an accuracy of almost 94%. PEG was seen to be primarily rresponsible for this high classification accuracy, singularly giving a result of aalmost 84% in a K-NN model. LPR was thus found to be not as strong a predictor, but nonetheless increased accuracy by around 10%  from an already high accuracy, thus proving to be of significance. In \"real world\" terms, performance on examinations directly related to the subject matter was an excellent predictor of student knowledge level while exam performance of subjects related to the main subject matter was shown to have a definite impact as well.\n",
    "    \n",
    "<li> Discussion of Expectations \n",
    "    \n",
    "We did expect student performance on subject matter related examinations (PEG) to be a good predictor of knowledge level as the examinations are directly designed to be assessments of student knowledge and student performance on the examinations is used to formally evaluate and classify student knowledge and learning outcomes. The effect of student performance on related items was interesting to see as we were not sure if knowledge and learning success would necessarily transfer between subjects. We had no particular expectations from the other attributes (update this proabably).\n",
    "    \n",
    "<li> Impact of Findings \n",
    "    \n",
    "The findings reinforce the expectation that student success on subject matter examinations can be used to identify student knowledge and this can be used to gather demographics for targeted interventions aimed at helping struggling students succeed, through analysis of exam scores. Exam performance on related items also seems to play a factor thus learning success may transfer at least partially between subjects and this can be used to formulate unique learning strategies on a case to case basis drawing on students' experiences in other topics and identify knowledge assimilation strategies used by students with higher knowledge levels. The lack of strong predictive value of the other attributes can be used to inform an emphasis on certain aspects of the learning process and evaluate specific related processes and programs in the knowledge transfer process where resources are targeted but predicitive value is not found.\n",
    "    \n",
    "<li> Future Questions\n",
    "    \n",
    "A similar analysis could be conducted, excluding PEG and LPR, to identify which of the other attributes are comparatively better at predicting UNS.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>References: \n",
    "\n",
    "H. T. Kahraman, Sagiroglu, S., Colak, I., Developing intuitive knowledge classifier and modeling of users' domain dependent data in web, Knowledge Based Systems, vol. 37, pp. 283-295, 2013.\n",
    "\n",
    "Zeina Bitar, Abbas Sandouk, Samih Al Jabi,\n",
    "Testing the Performances of DC Series Motor Used in Electric Car,\n",
    "Energy Procedia,\n",
    "Volume 74,\n",
    "2015,\n",
    "Pages 148-159,\n",
    "ISSN 1876-6102,\n",
    "https://doi.org/10.1016/j.egypro.2015.07.536.\n",
    "(https://www.sciencedirect.com/science/article/pii/S1876610215013041)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 002-08: Christopher Davis, Eashan Halbe, Moira Renata, Riley Lowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
